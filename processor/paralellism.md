Параллелизм — это способ выполнения вычислительных задач, при котором несколько операций или задач выполняются одновременно,
что позволяет существенно ускорить обработку данных и улучшить производительность систем.
Параллелизм можно рассматривать на нескольких уровнях:


Виды параллелизма

    Параллелизм на уровне битов: 
        Увеличение количества битов, которые процессор может обработать за одну операцию. 
        Например, переход от 32-битных к 64-битным процессорам.


    Параллелизм на уровне инструкций (ILP): 
        Способность процессора выполнять несколько инструкций одновременно, 
        используя конвейеры (pipelines) и суперскалярную архитектуру.


    Параллелизм на уровне данных (DLP): 
        Одновременная обработка одинаковых операций над массивами данных. 
        Примером является SIMD (Single Instruction, Multiple Data) в графических процессорах (GPU).


    Параллелизм на уровне задач (TLP): 
        Одновременное выполнение независимых задач или потоков. 
        Этот вид параллелизма реализуется с помощью многопоточности и многопроцессорности.



Уровни параллелизма

    Аппаратный уровень: 
        Использование многоядерных и многопроцессорных систем,
        где каждое ядро или процессор может выполнять свою собственную задачу.


    Уровень операционной системы: 
        Планирование и управление задачами таким образом, чтобы максимально эффективно использовать аппаратные ресурсы. 
        Операционные системы распределяют процессы и потоки по доступным ядрам процессора.


    Уровень приложений: 
        Разработка программ, способных выполнять несколько задач параллельно. 
        Это может быть реализовано с помощью многопоточности или распределенных вычислений.



Примеры и технологии параллелизма

    Многопоточность: 
        Позволяет программе выполнять несколько потоков одновременно. В языках программирования, таких как Java, Python и C++, 
        имеются встроенные библиотеки для создания и управления потоками.


    MPI (Message Passing Interface): 
        Используется в высокопроизводительных вычислениях для реализации параллелизма на уровне задач. 
        MPI позволяет процессам обмениваться сообщениями для координации выполнения.


    OpenMP: 
        Параллельное программирование на уровнях циклов и блоков кода. OpenMP предоставляет директивы для компиляторов, 
        которые позволяют разработчикам указывать параллельные участки кода.


    GPGPU (General-Purpose Computing on Graphics Processing Units): 
        Использование графических процессоров для выполнения общих вычислительных задач, 
        благодаря их способности обрабатывать тысячи потоков одновременно.


Пример параллелизма на Python с использованием библиотеки concurrent.futures

    
    from concurrent.futures import ThreadPoolExecutor
    import time
    
    def task(n):
        print(f"Задача {n} выполняется")
        time.sleep(1)
        return f"Задача {n} завершена"
    
    # Создаем пул потоков
    with ThreadPoolExecutor(max_workers=3) as executor:
        # Запускаем несколько задач параллельно
        futures = [executor.submit(task, i) for i in range(5)]
    
        # Ожидаем завершения задач и получаем результаты
        for future in futures:
            print(future.result())
    
    print("Все задачи завершены")
    
    В этом примере используется ThreadPoolExecutor для параллельного выполнения пяти задач. 
    Каждая задача выполняется в отдельном потоке, и все задачи выполняются одновременно, 
    что значительно сокращает общее время выполнения по сравнению с последовательным выполнением.


Параллелизм — это ключевая технология для ускорения вычислений и повышения производительности современных компьютерных систем.
